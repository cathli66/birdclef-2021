{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>filename</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['amegfi']</td>\n",
       "      <td>['begging call', 'call', 'juvenile']</td>\n",
       "      <td>35.3860</td>\n",
       "      <td>-84.1250</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>2012-08-12</td>\n",
       "      <td>XC109605.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>09:30</td>\n",
       "      <td>https://www.xeno-canto.org/109605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>9.1334</td>\n",
       "      <td>-79.6501</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Allen T. Chartier</td>\n",
       "      <td>2000-12-26</td>\n",
       "      <td>XC11209.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>?</td>\n",
       "      <td>https://www.xeno-canto.org/11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>5.7813</td>\n",
       "      <td>-75.7452</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Sergio Chaparro-Herrera</td>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>XC127032.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15:20</td>\n",
       "      <td>https://www.xeno-canto.org/127032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['whwbec1']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129974.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acafly</td>\n",
       "      <td>['whwbec1']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129981.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label secondary_labels                                  type  \\\n",
       "0        acafly       ['amegfi']  ['begging call', 'call', 'juvenile']   \n",
       "1        acafly               []                              ['call']   \n",
       "2        acafly               []                              ['call']   \n",
       "3        acafly      ['whwbec1']                              ['call']   \n",
       "4        acafly      ['whwbec1']                              ['call']   \n",
       "\n",
       "   latitude  longitude      scientific_name         common_name  \\\n",
       "0   35.3860   -84.1250  Empidonax virescens  Acadian Flycatcher   \n",
       "1    9.1334   -79.6501  Empidonax virescens  Acadian Flycatcher   \n",
       "2    5.7813   -75.7452  Empidonax virescens  Acadian Flycatcher   \n",
       "3    4.6717   -75.6283  Empidonax virescens  Acadian Flycatcher   \n",
       "4    4.6717   -75.6283  Empidonax virescens  Acadian Flycatcher   \n",
       "\n",
       "                       author        date      filename  \\\n",
       "0                 Mike Nelson  2012-08-12  XC109605.ogg   \n",
       "1           Allen T. Chartier  2000-12-26   XC11209.ogg   \n",
       "2     Sergio Chaparro-Herrera  2012-01-10  XC127032.ogg   \n",
       "3  Oscar Humberto Marin-Gomez  2009-06-19  XC129974.ogg   \n",
       "4  Oscar Humberto Marin-Gomez  2009-06-19  XC129981.ogg   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  09:30   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.0      ?   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     3.0  15:20   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "\n",
       "                                 url  \n",
       "0  https://www.xeno-canto.org/109605  \n",
       "1   https://www.xeno-canto.org/11209  \n",
       "2  https://www.xeno-canto.org/127032  \n",
       "3  https://www.xeno-canto.org/129974  \n",
       "4  https://www.xeno-canto.org/129981  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\n",
    "# from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "# from tensorflow.keras.applications import VGG19, VGG16, ResNet50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global vars\n",
    "RANDOM_SEED = 1337\n",
    "SAMPLE_RATE = 32000\n",
    "SIGNAL_LENGTH = 5 # seconds\n",
    "SPEC_SHAPE = (48, 128) # height x width\n",
    "FMIN = 500\n",
    "FMAX = 12500\n",
    "MAX_AUDIO_FILES = 1500\n",
    "\n",
    "\n",
    "path = '/opt/share/mldata/birdclef-data/'\n",
    "os.listdir(path)\n",
    "\n",
    "train = pd.read_csv('/opt/share/mldata/birdclef-data/train_metadata.csv',)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SPECIES IN TRAIN DATA: 132\n",
      "NUMBER OF SAMPLES IN TRAIN DATA: 22784\n",
      "LABELS: ['acowoo', 'amecro', 'amered', 'amerob', 'andsol1', 'astfly', 'azaspi1', 'banana', 'barant1', 'barswa', 'baywre1', 'bcnher', 'belvir', 'bewwre', 'bkbplo', 'bkcchi', 'bkhgro', 'bkmtou1', 'blbgra1', 'blujay', 'bncfly', 'bobfly1', 'brratt1', 'buggna', 'burwar1', 'butsal1', 'cangoo', 'carwre', 'caskin', 'ccbfin', 'chbant1', 'chcant2', 'chispa', 'clcrob', 'cobtan1', 'coltro1', 'compau', 'comrav', 'comyel', 'cubthr', 'daejun', 'ducfly', 'easmea', 'eastow', 'eucdov', 'eursta', 'fepowl', 'gbwwre1', 'gnwtea', 'grasal1', 'greegr', 'grekis', 'grethr1', 'grhowl', 'grtgra', 'grycat', 'gwfgoo', 'hergul', 'herthr', 'houfin', 'houspa', 'houwre', 'hutvir', 'indbun', 'killde', 'laufal1', 'lesgol', 'lesvio1', 'linspa', 'littin1', 'lobgna5', 'macwar', 'mallar3', 'marwre', 'meapar', 'mutswa', 'norcar', 'norfli', 'normoc', 'norwat', 'obnthr1', 'oliwoo1', 'orcwar', 'ovenbi1', 'pabspi1', 'pirfly1', 'plaxen1', 'plupig2', 'rcatan1', 'redcro', 'reevir1', 'rewbla', 'roahaw', 'ruboro1', 'rubpep1', 'ruckin', 'rucspa1', 'rucwar1', 'savspa', 'scbwre1', 'scptyr1', 'sltred', 'smbani', 'sobtyr1', 'socfly1', 'sonspa', 'soulap1', 'spotow', 'squcuc1', 'stejay', 'strcuc1', 'strfly1', 'sumtan', 'swathr', 'trokin', 'tropar', 'tuftit', 'warvir', 'wbwwre1', 'wesmea', 'wewpew', 'whbman1', 'whbnut', 'whcspa', 'whevir', 'whimbr', 'whtdov', 'whtspa', 'wlswar', 'woothr', 'yebcha', 'yeofly1']\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from: \n",
    "# https://www.kaggle.com/frlemarchand/bird-song-classification-using-an-efficientnet\n",
    "# Make sure to check out the entire notebook.\n",
    "\n",
    "# Limit the number of training samples and classes\n",
    "# First, only use high quality samples\n",
    "train = train.query('rating>=4')\n",
    "\n",
    "# Second, assume that birds with the most training samples are also the most common\n",
    "# A species needs at least 100 recordings with a rating above 4 to be considered common\n",
    "birds_count = {}\n",
    "for bird_species, count in zip(train.primary_label.unique(), \n",
    "                               train.groupby('primary_label')['primary_label'].count().values):\n",
    "    birds_count[bird_species] = count\n",
    "most_represented_birds = [key for key,value in birds_count.items() if value >= 100] \n",
    "\n",
    "TRAIN = train.query('primary_label in @most_represented_birds')\n",
    "LABELS = sorted(TRAIN.primary_label.unique())\n",
    "\n",
    "# Let's see how many species and samples we have left\n",
    "print('NUMBER OF SPECIES IN TRAIN DATA:', len(LABELS))\n",
    "print('NUMBER OF SAMPLES IN TRAIN DATA:', len(TRAIN))\n",
    "print('LABELS:', most_represented_birds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL NUMBER OF AUDIO FILES IN TRAINING DATA: 1500\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the training data and limit the number of audio files to MAX_AUDIO_FILES\n",
    "TRAIN = shuffle(TRAIN, random_state=RANDOM_SEED)[:MAX_AUDIO_FILES]\n",
    "\n",
    "# Define a function that splits an audio file, \n",
    "# extracts spectrograms and saves them in a working directory\n",
    "def get_spectrograms(filepath, primary_label, output_dir):\n",
    "    \n",
    "    # Open the file with librosa (limited to the first 15 seconds)\n",
    "    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)\n",
    "    \n",
    "    # Split signal into five second chunks\n",
    "    sig_splits = []\n",
    "    for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n",
    "        split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n",
    "\n",
    "        # End of signal?\n",
    "        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n",
    "            break\n",
    "        \n",
    "        sig_splits.append(split)\n",
    "        \n",
    "    # Extract mel spectrograms for each audio chunk\n",
    "    s_cnt = 0\n",
    "    saved_samples = []\n",
    "    for chunk in sig_splits:\n",
    "        \n",
    "        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n",
    "        mel_spec = librosa.feature.melspectrogram(y=chunk, \n",
    "                                                  sr=SAMPLE_RATE, \n",
    "                                                  n_fft=1024, \n",
    "                                                  hop_length=hop_length, \n",
    "                                                  n_mels=SPEC_SHAPE[0], \n",
    "                                                  fmin=FMIN, \n",
    "                                                  fmax=FMAX)\n",
    "    \n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
    "        \n",
    "        # Normalize\n",
    "        mel_spec -= mel_spec.min()\n",
    "        mel_spec /= mel_spec.max()\n",
    "        \n",
    "        # Save as image file\n",
    "        save_dir = os.path.join(output_dir, primary_label)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n",
    "                                 '_' + str(s_cnt) + '.png')\n",
    "        im = Image.fromarray(mel_spec * 255.0).convert(\"L\")\n",
    "        im.save(save_path)\n",
    "        \n",
    "        saved_samples.append(save_path)\n",
    "        s_cnt += 1\n",
    "        \n",
    "        \n",
    "    return saved_samples\n",
    "\n",
    "print('FINAL NUMBER OF AUDIO FILES IN TRAINING DATA:', len(TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data, noise_factor):\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    # Cast back to same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def shift(data, sampling_rate, shift_max, shift_direction):\n",
    "    shift = np.random.randint(sampling_rate * shift_max)\n",
    "    if shift_direction == 'right':\n",
    "        shift = -shift\n",
    "    elif shift_direction == 'both':\n",
    "        direction = np.random.randint(0, 2)\n",
    "        if direction == 1:\n",
    "            shift = -shift\n",
    "    augmented_data = np.roll(data, shift)\n",
    "    # Set to silence for heading/ tailing\n",
    "    if shift > 0:\n",
    "        augmented_data[:shift] = 0\n",
    "    else:\n",
    "        augmented_data[shift:] = 0\n",
    "    return augmented_data\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "\n",
    "def timestretch(data, speed_factor):\n",
    "    return librosa.effects.time_stretch(data, speed_factor)\n",
    "\n",
    "def get_augmented_specs(filepath, primary_label, output_dir):\n",
    "    # Open the file with librosa (limited to the first 15 seconds)\n",
    "    sig, rate = librosa.load(filepath, sr=SAMPLE_RATE, offset=None, duration=15)\n",
    "    \n",
    "    # randomly edit/augment audio files\n",
    "    aug_sig = noise(sig, random.randrange(0, 1))\n",
    "#     aug_sig = shift(aug_sig, rate, 2, 'both')\n",
    "    aug_sig = pitch(aug_sig, rate, random.randrange(-6, 6, 1))\n",
    "    aug_sig = timestretch(aug_sig, 0.5+random.randrange(0, 2))\n",
    "    \n",
    "\n",
    "    # Split augmented signals into five second chunks\n",
    "    sig_splits = []\n",
    "    for i in range(0, len(aug_sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n",
    "        split = aug_sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n",
    "\n",
    "        # End of signal?\n",
    "        if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n",
    "            break\n",
    "\n",
    "        sig_splits.append(split)\n",
    "        \n",
    "    # Extract mel spectrograms\n",
    "    s_cnt = 0\n",
    "    saved_samples = []\n",
    "    for chunk in sig_splits:\n",
    "\n",
    "        hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n",
    "        mel_spec = librosa.feature.melspectrogram(y=chunk, \n",
    "                                                  sr=SAMPLE_RATE, \n",
    "                                                  n_fft=1024, \n",
    "                                                  hop_length=hop_length, \n",
    "                                                  n_mels=SPEC_SHAPE[0], \n",
    "                                                  fmin=FMIN, \n",
    "                                                  fmax=FMAX)\n",
    "\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
    "\n",
    "        # Normalize\n",
    "        mel_spec -= mel_spec.min()\n",
    "        mel_spec /= mel_spec.max()\n",
    "\n",
    "        # Save as image file\n",
    "        save_dir = os.path.join(output_dir, primary_label)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, filepath.rsplit(os.sep, 1)[-1].rsplit('.', 1)[0] + \n",
    "                                 '_' + str(s_cnt) + '.png')\n",
    "        im = Image.fromarray(mel_spec * 255.0).convert(\"L\")\n",
    "        im.save(save_path)\n",
    "\n",
    "        saved_samples.append(save_path)\n",
    "        s_cnt += 1\n",
    "\n",
    "\n",
    "    return saved_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 722/1500 [18:53<21:18,  1.64s/it]"
     ]
    }
   ],
   "source": [
    "# Parse audio files and extract training samples, create augmented audio files\n",
    "input_dir = path+'/train_short_audio/'\n",
    "output_dir = '/cluster/2021cli/melspectrogram_dataset'\n",
    "samples = []\n",
    "with tqdm(total=len(TRAIN)) as pbar:\n",
    "    for idx, row in TRAIN.iterrows():\n",
    "        pbar.update(1)\n",
    "        \n",
    "#         if row.primary_label in most_represented_birds:\n",
    "        audio_file_path = os.path.join(input_dir, row.primary_label, row.filename)\n",
    "        samples += get_spectrograms(audio_file_path, row.primary_label, output_dir)\n",
    "        samples += get_augmented_specs(audio_file_path, row.primary_label, output_dir)\n",
    "\n",
    "            \n",
    "TRAIN_SPECS = shuffle(samples, random_state=RANDOM_SEED)\n",
    "print('SUCCESSFULLY EXTRACTED {} SPECTROGRAMS'.format(len(TRAIN_SPECS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 12 spectrograms of TRAIN_SPECS\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(12):\n",
    "    spec = Image.open(TRAIN_SPECS[i])\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.title(TRAIN_SPECS[i].split(os.sep)[-1])\n",
    "    plt.imshow(spec, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all samples and add spectrograms into train data, primary_labels into label data\n",
    "train_specs, train_labels = [], []\n",
    "with tqdm(total=len(TRAIN_SPECS)) as pbar:\n",
    "    for path in TRAIN_SPECS:\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Open image\n",
    "        spec = Image.open(path)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        spec = np.array(spec, dtype='float32')\n",
    "        \n",
    "        # Normalize between 0.0 and 1.0\n",
    "        # and exclude samples with nan \n",
    "        spec -= spec.min()\n",
    "        spec /= spec.max()\n",
    "        if not spec.max() == 1.0 or not spec.min() == 0.0:\n",
    "            continue\n",
    "\n",
    "        # Add channel axis to 2D array\n",
    "        spec = np.expand_dims(spec, -1)\n",
    "\n",
    "        # Add new dimension for batch size\n",
    "        spec = np.expand_dims(spec, 0)\n",
    "\n",
    "        # Add to train data\n",
    "        if len(train_specs) == 0:\n",
    "            train_specs = spec\n",
    "        else:\n",
    "            train_specs = np.vstack((train_specs, spec))\n",
    "\n",
    "        # Add to label data\n",
    "        target = np.zeros((len(LABELS)), dtype='float32')\n",
    "        bird = path.split(os.sep)[-2]\n",
    "        target[LABELS.index(bird)] = 1.0\n",
    "        if len(train_labels) == 0:\n",
    "            train_labels = target\n",
    "        else:\n",
    "            train_labels = np.vstack((train_labels, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your experiments are reproducible\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Build a simple model as a sequence of  convolutional blocks.\n",
    "# Each block has the sequence CONV --> RELU --> BNORM --> MAXPOOL.\n",
    "# Finally, perform global average pooling and add 2 dense layers.\n",
    "# The last layer is our classification layer and is softmax activated.\n",
    "# (Well it's a multi-label task so sigmoid might actually be a better choice)\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # First conv block\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', \n",
    "                           input_shape=(SPEC_SHAPE[0], SPEC_SHAPE[1], 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Second conv block\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)), \n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Third conv block\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)), \n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Fourth conv block\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Global pooling instead of flatten()\n",
    "    tf.keras.layers.GlobalAveragePooling2D(), \n",
    "    \n",
    "    # Dense block\n",
    "    tf.keras.layers.Dense(256, activation='relu'),   \n",
    "    tf.keras.layers.Dropout(0.5),  \n",
    "    tf.keras.layers.Dense(256, activation='relu'),   \n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Classification layer\n",
    "    tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])\n",
    "print('MODEL HAS {} PARAMETERS.'.format(model.count_params()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model and specify optimizer, loss and metric\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.004),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add callbacks to reduce the learning rate if needed, early stopping, and checkpoint saving\n",
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                  patience=2, \n",
    "                                                  verbose=1, \n",
    "                                                  factor=0.5),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              verbose=1,\n",
    "                                              patience=5),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', \n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=0,\n",
    "                                                save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model for a few epochs\n",
    "model.fit(train_specs, \n",
    "          train_labels,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks,\n",
    "          epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Pick a soundscape\n",
    "soundscape_path = '/opt/share/mldata/birdclef-data/train_soundscapes/28933_SSW_20170408.ogg'\n",
    "\n",
    "# Open it with librosa\n",
    "sig, rate = librosa.load(soundscape_path, sr=SAMPLE_RATE)\n",
    "\n",
    "# Store results so that we can analyze them later\n",
    "data = {'row_id': [], 'prediction': [], 'score': []}\n",
    "\n",
    "# Split signal into 5-second chunks\n",
    "# Just like we did before (well, this could actually be a seperate function)\n",
    "sig_splits = []\n",
    "for i in range(0, len(sig), int(SIGNAL_LENGTH * SAMPLE_RATE)):\n",
    "    split = sig[i:i + int(SIGNAL_LENGTH * SAMPLE_RATE)]\n",
    "\n",
    "    # End of signal?\n",
    "    if len(split) < int(SIGNAL_LENGTH * SAMPLE_RATE):\n",
    "        break\n",
    "\n",
    "    sig_splits.append(split)\n",
    "    \n",
    "# Get the spectrograms and run inference on each of them\n",
    "# This should be the exact same process as we used to\n",
    "# generate training samples!\n",
    "seconds, scnt = 0, 0\n",
    "for chunk in sig_splits:\n",
    "    \n",
    "    # Keep track of the end time of each chunk\n",
    "    seconds += 5\n",
    "        \n",
    "    # Get the spectrogram\n",
    "    hop_length = int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n",
    "    mel_spec = librosa.feature.melspectrogram(y=chunk, \n",
    "                                              sr=SAMPLE_RATE, \n",
    "                                              n_fft=1024, \n",
    "                                              hop_length=hop_length, \n",
    "                                              n_mels=SPEC_SHAPE[0], \n",
    "                                              fmin=FMIN, \n",
    "                                              fmax=FMAX)\n",
    "\n",
    "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n",
    "\n",
    "    # Normalize to match the value range we used during training.\n",
    "    # That's something you should always double check!\n",
    "    mel_spec -= mel_spec.min()\n",
    "    mel_spec /= mel_spec.max()\n",
    "    \n",
    "    # Add channel axis to 2D array\n",
    "    mel_spec = np.expand_dims(mel_spec, -1)\n",
    "\n",
    "    # Add new dimension for batch size\n",
    "    mel_spec = np.expand_dims(mel_spec, 0)\n",
    "    \n",
    "    # Predict\n",
    "    p = model.predict(mel_spec)[0]\n",
    "    \n",
    "    # Get highest scoring species\n",
    "    idx = p.argmax()\n",
    "    species = LABELS[idx]\n",
    "    score = p[idx]\n",
    "    \n",
    "    # Prepare submission entry\n",
    "    data['row_id'].append(soundscape_path.split(os.sep)[-1].rsplit('_', 1)[0] + \n",
    "                          '_' + str(seconds))    \n",
    "    \n",
    "    # Decide if it's a \"nocall\" or a species by applying a threshold\n",
    "    if score > 0.25:\n",
    "        data['prediction'].append(species)\n",
    "        scnt += 1\n",
    "    else:\n",
    "        data['prediction'].append('nocall')\n",
    "        \n",
    "    # Add the confidence score as well\n",
    "    data['score'].append(score)\n",
    "        \n",
    "print('SOUNSCAPE ANALYSIS DONE. FOUND {} BIRDS.'.format(scnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new data frame\n",
    "results = pd.DataFrame(data, columns = ['row_id', 'prediction', 'score'])\n",
    "\n",
    "# Merge with ground truth so we can inspect\n",
    "gt = pd.read_csv('/opt/share/mldata/birdclef-data/train_soundscape_labels.csv',)\n",
    "results = pd.merge(gt, results, on='row_id')\n",
    "\n",
    "# Let's look at the first 50 entries\n",
    "results.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data, columns = ['row_id', 'prediction'])\n",
    "# Convert our results to csv\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BirdCLEF",
   "language": "python",
   "name": "birdclef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
